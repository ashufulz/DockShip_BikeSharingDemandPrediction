# -*- coding: utf-8 -*-
"""DockShip_BikeSharingDemand.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NqkPD3gbuuwiWn9skeqvpcld6DSd6TFr

# **The aim of the challenge is to predict the hourly demand for rental bikes using the given dataset.**
"""

# Downloading the dataset
# !wget -O "bike_sharing_demand_prediction_ai_challenge_-dataset.zip" "https://dockship-job-models.s3.ap-south-1.amazonaws.com/6e6865573bee88d8e4bf5285a706f078?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIDOPTEUZ2LEOQEGQ%2F20210206%2Fap-south-1%2Fs3%2Faws4_request&X-Amz-Date=20210206T133942Z&X-Amz-Expires=1800&X-Amz-Signature=63fb959502ace26a0d3e0a7d4c7bb6fb93d5cf9e4f6a77c01f45321f80e87a4f&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3D%22bike_sharing_demand_prediction_ai_challenge_-dataset.zip%22"

# Importing libraries
import pandas as pd

# To display all the columns
pd.set_option('display.max_columns', None)

# Loading the dataset into the dataframe
df = pd.read_csv('/content/drive/MyDrive/DataScience/Dataset/_CHALLENGES_/DockShip/bike_sharing_demand_prediction_ai_challenge_-dataset.zip (Unzipped Files)/TRAIN.csv')

# Setting the index parameter
df.set_index('Index', inplace=True)

df.shape

df.head()

# Checking the datatypes of the variables
df.info()

"""### Here we can see that there are many different datatypes. So we'll convert all the 'object' dtypes into int/float soon."""

# Getting more detailes insights about the data
df.describe()

# Checking missing values (if available)
df.isna().sum()

df.head()

"""# DATA PREPROCESSING
###Let's take the columns one by one

###Let's start with 'Date' column
### We would not need the year, only date and month would help in prediction. Hence we'll extract only date and month from it.
"""

df['Date_Day'] = pd.to_datetime(df.Date, format='%d/%m/%Y').dt.day            # Extracting the day
df['Date_Month'] = pd.to_datetime(df.Date, format='%d/%m/%Y').dt.month        # Extracting the month

df.head()

# Since we have converted 'Date' column to integer values it is of no use, hence we'll drop it
df.drop(['Date'], axis=1, inplace=True)

df.head()

# Checking the categorical columns
print('Seasons:', df.Seasons.unique())
print('Holiday:', df.Holiday.unique())
print('Func Day:', df['Functioning Day'].unique())

df.Seasons.value_counts()

df.Holiday.value_counts()

df['Functioning Day'].value_counts()

# Importing Visualizing libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Visualizing Seasons vs Bike Demand
plt.figure(figsize=(20, 10))
sns.boxplot(x='Seasons', y='Rented Bike Count', data=df.sort_values('Rented Bike Count', ascending=False))
plt.show()

"""#### As seen, in Winter season the demand for bikes falls down"""

# Visualizing Holiday vs Bike Demand
plt.figure(figsize=(16, 10))
sns.boxplot(x='Holiday', y='Rented Bike Count', data=df.sort_values('Rented Bike Count', ascending=False))
plt.show()

# Visualizing 'Functioning Day' vs Bike Demand
plt.figure(figsize=(16, 10))
sns.boxplot(x='Functioning Day', y='Rented Bike Count', data=df.sort_values('Rented Bike Count', ascending=False))
plt.show()

"""### Now we have here three categorical columns namely 'Seasons', 'Holiday' and 'Functioning Day'

### As the categorical columns are random, we can't go with LabelEncoding on it.
### We'll use OneHotEncoding
Nominal data -> data is random -> OneHotEncoding
"""

# One hot encoding on categorical columns.
df = pd.get_dummies(df, columns=['Seasons','Holiday', 'Functioning Day'], drop_first=True)

df.head()

"""### Now we have all the columns in numerical format, so we can visualize the correlation in a better way."""

plt.figure(figsize=(25, 25))
sns.heatmap(df.corr(), annot=True, cmap='Blues')

"""### Defining the variables"""

X = df.drop(['Rented Bike Count'], axis=1)        # Independant Variable / Features
y = df['Rented Bike Count']                       # Dependant Variable / Target

"""### Getting important features using ExtraTreesRegressor

"""

from sklearn.ensemble import ExtraTreesRegressor
model = ExtraTreesRegressor()
model.fit(X, y)

model.feature_importances_

"""### Visualizing Important Features"""

plt.figure(figsize=(12,8))
impft = pd.Series(model.feature_importances_, index=X.columns)
impft.nlargest(20).plot(kind='barh')
plt.show()

"""# BUILDING MODEL"""

# Dividing the data into train and test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)

"""##**. LINEAR REGRESSION**"""

from sklearn.linear_model import LinearRegression

mod_lin = LinearRegression()
mod_lin.fit(X_train, y_train)

"""### *Getting the prediction*"""

ypred_lin = mod_lin.predict(X_test)
ypred_lin

# As only RMSE will be taken into account, we'll import that
from sklearn.metrics import mean_squared_error as mse
from math import sqrt as r

# Metrics for Linear Regression

rmse_lin = r(mse(y_test, ypred_lin))
print('RMSE Lin:', rmse_lin)

"""## **. DECISION TREES**

### *Finding best parameters for Decision Tree*
"""

from sklearn.tree import DecisionTreeRegressor
import numpy as np

dt = DecisionTreeRegressor(random_state=42)
dt_params = {'max_depth':np.arange(1,50,2), 'min_samples_leaf':np.arange(2,15)}

from sklearn.model_selection import GridSearchCV

mod_gs_dt = GridSearchCV(dt, dt_params, cv=3)
mod_gs_dt.fit(X_train,y_train)
bp_dt = mod_gs_dt.best_params_
bp_dt

"""### *Training with best parameters*"""

dtr = DecisionTreeRegressor(max_depth = bp_dt['max_depth'], min_samples_leaf = bp_dt['min_samples_leaf'])
mod_dtr = dtr.fit(X_train, y_train)
ypred_dtr = mod_dtr.predict(X_test)

# Metrics for Decision Tree Regression

rmse_dtr = r(mse(y_test, ypred_dtr))
print('RMSE DTR:', rmse_dtr)

"""## **. RANDOM FOREST**

### *Finding best parameters for RandomForestRegressor*
"""

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(random_state=42)
rf_params = {'n_estimators':np.arange(25,150,25), 'max_depth':np.arange(1,11,2), 'min_samples_leaf':np.arange(2,15,3)}

mod_gs_rf = GridSearchCV(rf, rf_params, cv=3)
mod_gs_rf.fit(X_train, y_train)
bp_rf = mod_gs_rf.best_params_
bp_rf

"""### *Fitting the model with best params*"""

rfr = RandomForestRegressor(n_estimators=bp_rf['n_estimators'], max_depth=bp_rf['max_depth'], min_samples_leaf=bp_rf['min_samples_leaf'], random_state=42)
mod_rfr = rfr.fit(X_train, y_train)
ypred_rfr = mod_rfr.predict(X_test)

ypred_rfr

# Metrics for Decision Random Forest

rmse_rfr = r(mse(y_test, ypred_rfr))
print('RMSE RFR:', rmse_rfr)

"""## **. ADA - BOOST**

### *Finding best parameters for AdaBoostRegressor*
"""

from sklearn.ensemble import AdaBoostRegressor

ab = AdaBoostRegressor(base_estimator=rfr, random_state=42)
ab_params = {'n_estimators':np.arange(25,200,25)}

gs_ab = GridSearchCV(ab, ab_params, cv=3)
gs_ab.fit(X_train, y_train)
bp_ab = gs_ab.best_params_
bp_ab

"""### *Fitting the model with best params*"""

abr = AdaBoostRegressor(base_estimator=rfr, n_estimators=bp_ab['n_estimators'], random_state=42)
mod_abr = abr.fit(X_train, y_train)
ypred_abr = mod_abr.predict(X_test)

ypred_abr

# Metrics for Ada Boost Regressor

rmse_abr = r(mse(y_test, ypred_abr))
print('RMSE ABR:', rmse_abr)

"""## Comparing all the RMSE values:"""

print('RMSE_DTR:', rmse_dtr)
print('RMSE_RFR:', rmse_rfr)
print('RMSE_ABR:', rmse_abr)

"""### The lesser the score the better. Here as seen in above comparison, the RMSE score of AdaBoostRegressor is the closer to zero than any other, hence we'll use the to test our model.

#TEST DATA
"""

test_df = pd.read_csv('/content/drive/MyDrive/DataScience/Dataset/_CHALLENGES_/DockShip/bike_sharing_demand_prediction_ai_challenge_-dataset.zip (Unzipped Files)/TEST.csv')

# Setting the index parameter
test_df.set_index('Index', inplace=True)

test_df.head()

"""## Applying all the same preprocessing stuff on Test data

### Processing the Date column
"""

test_df['Date_Day'] = pd.to_datetime(test_df.Date, format='%d/%m/%Y').dt.day            # Extracting the day
test_df['Date_Month'] = pd.to_datetime(test_df.Date, format='%d/%m/%Y').dt.month        # Extracting the month

test_df.drop(['Date'], axis=1, inplace=True)

"""### OneHotEncoding the categorical columns"""

# Checking the categorical columns first
print(test_df['Seasons'].value_counts())
print(test_df['Holiday'].value_counts())
print(test_df['Functioning Day'].value_counts())

"""### As seen here the Seasons column has only one subcategory. So we need to do OHE in a different way here."""

test_df = pd.get_dummies(test_df, columns=['Seasons'])

# Now performing OHE on the rest two columns
test_df = pd.get_dummies(test_df, columns=['Holiday', 'Functioning Day'], drop_first=True)

test_df.head()

test_df.info()

# Converting 'uint8' into integer datatype
test_df['Seasons_Autumn'] = test_df['Seasons_Autumn'].astype(int)
test_df['Holiday_No Holiday'] = test_df['Holiday_No Holiday'].astype(int)
test_df['Functioning Day_Yes'] = test_df['Functioning Day_Yes'].astype(int)

"""### As AdaBoostRegressor has the lowest RMSE score, we'll use it on the test data"""

import pickle

# Saving the model
abr_model = 'abr_model.sav'
pickle.dump(mod_abr, open(abr_model, 'wb'))

# Loading the saved model
abr_model_loaded = pickle.load(open(abr_model, 'rb'))

# Testing on the Test Dataset
test_mod_abr = abr_model_loaded.fit(X_train, y_train)
test_ypred = test_mod_abr.predict(X_test)

test_df['Rented Bike Count'] = test_ypred[:1728,].astype(int)

test_df.head()

test_df = test_df['Rented Bike Count'].to_csv('output.csv', index=True, header=True)

